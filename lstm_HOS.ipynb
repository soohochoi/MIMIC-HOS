{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules and set configurations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os, copy, random, pickle, gc\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import torch\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'df_std_hos_dict_6hour.pkl', 'rb') as f:\n",
    "    df_raw_icu_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_raw_icu_dict.keys():\n",
    "    df_raw_icu_dict[i] = {\n",
    "        'hadm_id': df_raw_icu_dict[i]['hadm_id'].to_numpy().reshape(-1,83,1),\n",
    "        'X': df_raw_icu_dict[i].drop(labels=['hadm_id', 'time'], axis=1).to_numpy().reshape(-1,83,383)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:47<00:00, 11.99s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F \n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "data_loaders = {}\n",
    "\n",
    "for i in tqdm(df_raw_icu_dict.keys()):\n",
    "    tmp_X = torch.tensor(df_raw_icu_dict[i]['X'].astype(np.float32))\n",
    "    tmp_ids = torch.tensor(df_raw_icu_dict[i]['hadm_id'].astype(int))\n",
    "    if i in ['train_x', 'val_x']:\n",
    "        data_loaders[i] = DataLoader(dataset=TensorDataset(tmp_X, tmp_ids), batch_size=128, shuffle=False)\n",
    "    else:\n",
    "        data_loaders[i] = DataLoader(dataset=TensorDataset(tmp_X, tmp_ids), batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, seq_len, n_features, embedding_dim):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.seq_len, self.n_features = seq_len, n_features\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.hidden_dim =  2 * embedding_dim\n",
    "    self.rnn1 = nn.LSTM(\n",
    "      input_size=n_features,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "    self.rnn2 = nn.LSTM(\n",
    "      input_size=self.hidden_dim,\n",
    "      hidden_size=embedding_dim,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x, (_, _) = self.rnn1(x)\n",
    "    x, (hidden_n, _) = self.rnn2(x)\n",
    "    return hidden_n.reshape((-1,1, self.embedding_dim))\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, seq_len, input_dim, n_features=383):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.seq_len, self.input_dim = seq_len, input_dim\n",
    "    self.hidden_dim = 2 * input_dim\n",
    "    self.n_features = n_features\n",
    "    self.rnn1 = nn.LSTM(\n",
    "      input_size=input_dim,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "    self.output_layer = nn.Linear(self.hidden_dim, n_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.repeat(1,self.seq_len, 1)\n",
    "    x, (hidden_n, cell_n) = self.rnn1(x)\n",
    "    return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentAutoencoder(nn.Module):\n",
    "  def __init__(self, seq_len, n_features, embedding_dim):\n",
    "    super(RecurrentAutoencoder, self).__init__()\n",
    "    self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
    "    self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    x = self.decoder(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training setting\n",
    "seed_everything(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "hidden_unit = 512 # 64, 128, 256, 512\n",
    "n_epochs, factor, patience, min_lr = (3000, 0.1, 100, 1e-6)\n",
    "loss_reduction = 'stay_wise_mean'\n",
    "early_patience=200\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = RecurrentAutoencoder(83, 383, hidden_unit)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=patience, min_lr=min_lr, verbose=True)\n",
    "criterion = nn.MSELoss(reduction='none').to(device)\n",
    "\n",
    "history = dict(train=[], val=[])\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_loss = float('inf')\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='std' #raw, std, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 18819.182555786865 val loss 21744.560444078947\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 2: train loss 17085.014364094077 val loss 17506.47583650288\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 3: train loss 12504.412716071763 val loss 15120.38655491879\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 4: train loss 10842.952055240106 val loss 13735.885604055304\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 5: train loss 9628.445900922763 val loss 13034.112458881578\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 6: train loss 8665.005637528653 val loss 12245.87826538086\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 7: train loss 7760.64486200938 val loss 11308.74271432977\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 8: train loss 7052.687635958551 val loss 10784.972810444078\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 9: train loss 6605.009390642543 val loss 10311.504945453844\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 10: train loss 6093.719850825692 val loss 10259.41822734632\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 11: train loss 5668.980915366533 val loss 9514.699531153628\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 12: train loss 5286.838589788197 val loss 9298.912477593673\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 13: train loss 5017.331738135058 val loss 9126.775615892911\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 14: train loss 4743.854285325832 val loss 8836.064506129214\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 15: train loss 4548.850621754538 val loss 8716.938753228438\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 16: train loss 4484.380302018034 val loss 8425.153211493242\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 17: train loss 4216.066475691196 val loss 8099.650313527961\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 18: train loss 4194.707989721241 val loss 8198.585301047877\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 19: train loss 4081.629361455312 val loss 8380.59833566766\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 20: train loss 3891.104425487404 val loss 7459.08405866121\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 21: train loss 3792.6618396507765 val loss 7295.337624399285\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 22: train loss 3680.051613082429 val loss 7122.21628931949\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 23: train loss 3671.2584475214608 val loss 7817.162284851074\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 24: train loss 3716.5411069949946 val loss 7186.00958412572\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 25: train loss 3403.119038656086 val loss 6886.884384958367\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 26: train loss 3259.3307882525964 val loss 6906.4802808259665\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 27: train loss 3295.263707326558 val loss 7037.734181454307\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 28: train loss 3270.606292907349 val loss 6689.723592256245\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 29: train loss 3189.512713403759 val loss 6447.955982810573\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 30: train loss 3047.681590919723 val loss 6191.087865729081\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 31: train loss 2940.6615907589116 val loss 6219.351324382581\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 32: train loss 2900.745505327236 val loss 6534.285877830104\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 33: train loss 2890.7705849287754 val loss 6636.547648379677\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 34: train loss 2870.7891642861737 val loss 7220.52532878675\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 35: train loss 2960.3372773495976 val loss 7059.902654948987\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 36: train loss 2739.4721590144904 val loss 5771.391641717208\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 37: train loss 2959.90868994433 val loss 7214.561774805972\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 38: train loss 2728.8732166404498 val loss 5734.699293839304\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 39: train loss 2580.2978256134215 val loss 5842.787749039499\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 40: train loss 2542.4875226963068 val loss 6059.875929581492\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 41: train loss 2614.6773049360263 val loss 6244.283897399902\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 42: train loss 2578.974297072359 val loss 5644.519570601614\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 43: train loss 2525.464155916682 val loss 5738.138880277935\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 44: train loss 2640.926158608077 val loss 7249.8439865112305\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 45: train loss 3248.238573268502 val loss 6158.429297196238\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 46: train loss 2856.95654735451 val loss 7520.283179835269\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 47: train loss 2600.7759870883233 val loss 6760.03854530736\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 48: train loss 2479.5016682333576 val loss 7558.647129661159\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 49: train loss 2389.659158672401 val loss 7417.4997948094415\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 50: train loss 2348.883314669489 val loss 7064.8207465723945\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 51: train loss 2330.463135862065 val loss 6117.726959630063\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 52: train loss 2331.7496454844218 val loss 7064.042769582648\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 53: train loss 2441.300844660776 val loss 7272.537618135151\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 54: train loss 2414.093355281624 val loss 7349.71309139854\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 12 \n",
      "Epoch 55: train loss 2299.3622648501823 val loss 6993.510129426655\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 13 \n",
      "Epoch 56: train loss 2309.4537669655806 val loss 6569.163838035182\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 14 \n",
      "Epoch 57: train loss 2293.7167086115855 val loss 5196.271748592979\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 58: train loss 2244.7232662360825 val loss 5179.986614026521\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 59: train loss 2190.0134673889524 val loss 5128.639786168149\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 60: train loss 2085.389746660244 val loss 5703.089134617856\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 61: train loss 2084.7622764724456 val loss 5412.814522592645\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 62: train loss 2000.1470899753228 val loss 5326.569818998638\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 63: train loss 1999.2469105977498 val loss 6316.456628899825\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 64: train loss 2018.128288314728 val loss 5146.452052869295\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 65: train loss 2152.610055158238 val loss 8607.503271404066\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 66: train loss 2593.3748474121094 val loss 6558.655088324296\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 67: train loss 2240.964273966715 val loss 5314.044479370117\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 68: train loss 2014.463020644502 val loss 6620.290272361354\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 69: train loss 1931.5380778969404 val loss 6425.932207609478\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 70: train loss 1972.276498303442 val loss 5192.89628641229\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 71: train loss 1871.7073264664518 val loss 5203.874512923391\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 12 \n",
      "Epoch 72: train loss 1826.315023365135 val loss 5292.2310052169\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 13 \n",
      "Epoch 73: train loss 1820.2213464611305 val loss 5821.549018859863\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 14 \n",
      "Epoch 74: train loss 1825.8697559105422 val loss 6443.955815766987\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 15 \n",
      "Epoch 75: train loss 1803.8401578800408 val loss 5522.420844630191\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 16 \n",
      "Epoch 76: train loss 1802.7042867694786 val loss 6685.719977529426\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 17 \n",
      "Epoch 77: train loss 1780.8921143309085 val loss 5891.545044748406\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 18 \n",
      "Epoch 78: train loss 1948.3245490525296 val loss 5894.726039685701\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 19 \n",
      "Epoch 79: train loss 1897.8457931244445 val loss 6983.564321417558\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 20 \n",
      "Epoch 80: train loss 1927.3941675060523 val loss 7209.559987921464\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 21 \n",
      "Epoch 81: train loss 1976.7735804940412 val loss 6941.429486324912\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 22 \n",
      "Epoch 82: train loss 1771.1906957569236 val loss 6563.761787414551\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 23 \n",
      "Epoch 83: train loss 1777.6956465486994 val loss 7031.978404798006\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 24 \n",
      "Epoch 84: train loss 1727.600392050372 val loss 6766.2883943256575\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 25 \n",
      "Epoch 85: train loss 1733.201642613211 val loss 6596.2294845581055\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 26 \n",
      "Epoch 86: train loss 1711.6933361670215 val loss 6812.592371488872\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 27 \n",
      "Epoch 87: train loss 1657.9694286963183 val loss 6833.855735377261\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 28 \n",
      "Epoch 88: train loss 1628.7425196299296 val loss 6800.78408211156\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 29 \n",
      "Epoch 89: train loss 1625.948438632988 val loss 6822.191060116416\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 30 \n",
      "Epoch 90: train loss 1708.5201432462225 val loss 6775.8437829268605\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 31 \n",
      "Epoch 91: train loss 1707.8149427767999 val loss 6817.036840338456\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 32 \n",
      "Epoch 92: train loss 1658.5708141212692 val loss 6655.009902954102\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 33 \n",
      "Epoch 93: train loss 1605.4261735013858 val loss 6934.772388257478\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 34 \n",
      "Epoch 94: train loss 1554.4775770724177 val loss 6748.6946362947165\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 35 \n",
      "Epoch 95: train loss 1534.9373250264607 val loss 6924.209215264571\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 36 \n",
      "Epoch 96: train loss 1606.7553530024911 val loss 6843.240382144326\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 37 \n",
      "Epoch 97: train loss 1625.458590935804 val loss 6814.628464949758\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 38 \n",
      "Epoch 98: train loss 1602.1023301906928 val loss 6607.587648893657\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 39 \n",
      "Epoch 99: train loss 1597.4999742336615 val loss 6780.012219880757\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 40 \n",
      "Epoch 100: train loss 1908.83251880029 val loss 6932.615514253315\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 41 \n",
      "Epoch 101: train loss 1708.9685134430845 val loss 6820.482141595137\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 42 \n",
      "Epoch 102: train loss 1637.0384695087364 val loss 6811.669538799085\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 43 \n",
      "Epoch 103: train loss 1507.2356585656812 val loss 6666.203591999255\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 44 \n",
      "Epoch 104: train loss 1452.4986926781203 val loss 6614.859155755294\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 45 \n",
      "Epoch 105: train loss 1467.7075915308055 val loss 7020.094452707391\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 46 \n",
      "Epoch 106: train loss 1441.6765159561248 val loss 6818.21650655646\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 47 \n",
      "Epoch 107: train loss 1405.1201760297763 val loss 6854.007934168765\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 48 \n",
      "Epoch 108: train loss 1417.7974170981768 val loss 6910.233673898797\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 49 \n",
      "Epoch 109: train loss 1448.4193432288255 val loss 6866.898197374846\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 50 \n",
      "Epoch 110: train loss 1773.8560947258316 val loss 6992.395464445415\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 51 \n",
      "Epoch 111: train loss 1600.0538150992936 val loss 7001.795153166118\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 52 \n",
      "Epoch 112: train loss 1717.702097818523 val loss 6958.438608269942\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 53 \n",
      "Epoch 113: train loss 1531.6561312190072 val loss 6497.836663095574\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 54 \n",
      "Epoch 114: train loss 1476.9137566846289 val loss 6627.200263575503\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 55 \n",
      "Epoch 115: train loss 1390.6165285396005 val loss 6653.952982450786\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 56 \n",
      "Epoch 116: train loss 1352.795784042267 val loss 6775.812388369912\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 57 \n",
      "Epoch 117: train loss 1565.2910117874603 val loss 7579.988715723941\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 58 \n",
      "Epoch 118: train loss 1606.3051459032617 val loss 6115.177751239978\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 59 \n",
      "Epoch 119: train loss 1424.3812773013544 val loss 6760.853542127107\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 60 \n",
      "Epoch 120: train loss 1383.8959349672239 val loss 6941.199782120554\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 61 \n",
      "Epoch 121: train loss 1354.509458347709 val loss 6844.164391768606\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 62 \n",
      "Epoch 122: train loss 1390.06901239635 val loss 6568.290775499846\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 63 \n",
      "Epoch 123: train loss 1329.434390770461 val loss 7068.954696655273\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 64 \n",
      "Epoch 124: train loss 1361.2172248520537 val loss 6766.8296653346015\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 65 \n",
      "Epoch 125: train loss 1372.592399825593 val loss 6168.149171528064\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 66 \n",
      "Epoch 126: train loss 1298.8665091691616 val loss 6502.0854269328875\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 67 \n",
      "Epoch 127: train loss 1275.3950544345878 val loss 5199.740827460038\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 68 \n",
      "Epoch 128: train loss 1255.3974984905676 val loss 5002.171533684981\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 129: train loss 1293.9798425914285 val loss 4871.168041831569\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 130: train loss 1354.036505876187 val loss 4776.890367608321\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 131: train loss 1342.2730263350252 val loss 6360.710469697651\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 132: train loss 1316.8842573337213 val loss 4900.581700776753\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 133: train loss 1280.5792412672215 val loss 5057.613828157124\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 134: train loss 1258.94775646461 val loss 5539.23929636102\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 135: train loss 1249.9306906511683 val loss 6083.158484609504\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 136: train loss 1336.6625297683443 val loss 4830.704184682746\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 137: train loss 1294.3202284396052 val loss 5909.273747494346\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 138: train loss 1264.717902908782 val loss 4933.181985152395\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 139: train loss 1232.477231602469 val loss 6057.277257818925\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 140: train loss 1183.9746620954868 val loss 5100.829450506913\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 141: train loss 1200.3964846491099 val loss 5421.496070861816\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 142: train loss 1172.0564947071189 val loss 4837.122246993215\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 12 \n",
      "Epoch 143: train loss 1160.7964653883153 val loss 5628.250725194028\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 13 \n",
      "Epoch 144: train loss 1147.3944904989826 val loss 4766.77551309686\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 145: train loss 1150.1715547481697 val loss 6133.921450163189\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 146: train loss 1226.4053336503264 val loss 6579.7282060322\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 147: train loss 1210.0834662157617 val loss 6479.659702903346\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 148: train loss 1148.5729921078253 val loss 5364.384785702354\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 149: train loss 1133.2416304171443 val loss 5591.747243379292\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 150: train loss 1115.3338391880789 val loss 4819.763242219624\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 151: train loss 1109.6905709455114 val loss 5070.667070890728\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 152: train loss 1093.0575128429664 val loss 4915.40868176912\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 153: train loss 1113.6971791889853 val loss 6744.756267346834\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 154: train loss 1451.1729390949547 val loss 5071.684228595935\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 155: train loss 1276.8603034105129 val loss 5316.207386217619\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 156: train loss 1149.804116711645 val loss 5137.3603844893605\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 12 \n",
      "Epoch 157: train loss 1112.054882666308 val loss 6854.39712805497\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 13 \n",
      "Epoch 158: train loss 1083.802932282408 val loss 6455.539661608244\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 14 \n",
      "Epoch 159: train loss 1087.577927640812 val loss 6324.648584064685\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 15 \n",
      "Epoch 160: train loss 1080.109392634409 val loss 6229.262847900391\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 16 \n",
      "Epoch 161: train loss 1075.9497655080463 val loss 6695.021539788497\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 17 \n",
      "Epoch 162: train loss 1058.78336768236 val loss 6579.81429089998\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 18 \n",
      "Epoch 163: train loss 1020.6774728740761 val loss 6529.0373205887645\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 19 \n",
      "Epoch 164: train loss 1000.7920693951453 val loss 6214.66300763582\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 20 \n",
      "Epoch 165: train loss 1014.8244031346487 val loss 6820.735304581492\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 21 \n",
      "Epoch 166: train loss 1023.7317000520444 val loss 5484.93096803364\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 22 \n",
      "Epoch 167: train loss 1065.220558943149 val loss 6190.83808858771\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 23 \n",
      "Epoch 168: train loss 1004.0195684375877 val loss 6151.589707223992\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 24 \n",
      "Epoch 169: train loss 1000.9497849698552 val loss 6561.146477548699\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 25 \n",
      "Epoch 170: train loss 994.8689896817692 val loss 6550.641526874743\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 26 \n",
      "Epoch 171: train loss 1007.270898510596 val loss 6085.938988133481\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 27 \n",
      "Epoch 172: train loss 1021.9973314479439 val loss 5767.458820945339\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 28 \n",
      "Epoch 173: train loss 1088.3673330524011 val loss 6384.977874354312\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 29 \n",
      "Epoch 174: train loss 1022.7567123138977 val loss 6567.989647714715\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 30 \n",
      "Epoch 175: train loss 984.0871629086797 val loss 5984.08685222425\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 31 \n",
      "Epoch 176: train loss 972.2099629019549 val loss 6179.813532377544\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 32 \n",
      "Epoch 177: train loss 989.6306346048138 val loss 4798.331981458162\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 33 \n",
      "Epoch 178: train loss 969.9038044821002 val loss 5018.839550219084\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 34 \n",
      "Epoch 179: train loss 993.6634247374392 val loss 5652.004351967259\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 35 \n",
      "Epoch 180: train loss 1027.9032126329616 val loss 6809.04335945531\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 36 \n",
      "Epoch 181: train loss 984.9390539751795 val loss 6336.1184724506575\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 37 \n",
      "Epoch 182: train loss 962.2241820832213 val loss 5751.122956526907\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 38 \n",
      "Epoch 183: train loss 952.4818390258058 val loss 4975.832240857576\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 39 \n",
      "Epoch 184: train loss 941.4933506400286 val loss 6160.900958814119\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 40 \n",
      "Epoch 185: train loss 963.4585452964919 val loss 5937.908143696032\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 41 \n",
      "Epoch 186: train loss 927.5853708232949 val loss 6720.85455964741\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 42 \n",
      "Epoch 187: train loss 1208.994532236796 val loss 7063.425140380859\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 43 \n",
      "Epoch 188: train loss 1616.5322456588287 val loss 6465.113111395585\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 44 \n",
      "Epoch 189: train loss 1186.8552605634677 val loss 6467.849498548006\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 45 \n",
      "Epoch 190: train loss 1143.957453470744 val loss 6676.7859127647\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 46 \n",
      "Epoch 191: train loss 1003.9698650337265 val loss 6544.90558021947\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 47 \n",
      "Epoch 192: train loss 1036.082521312965 val loss 6615.575080871582\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 48 \n",
      "Epoch 193: train loss 927.5088021284092 val loss 6666.241204111199\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 49 \n",
      "Epoch 194: train loss 953.3150848114562 val loss 6754.494714034231\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 50 \n",
      "Epoch 195: train loss 943.1318231457008 val loss 6704.084954914294\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 51 \n",
      "Epoch 196: train loss 914.5122258991538 val loss 6765.902654547441\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 52 \n",
      "Epoch 197: train loss 882.2374374024168 val loss 6686.383932816355\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 53 \n",
      "Epoch 198: train loss 886.293016719247 val loss 6765.357153240003\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 54 \n",
      "Epoch 199: train loss 855.4859558836429 val loss 5641.678283289859\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 55 \n",
      "Epoch 200: train loss 860.1500901090884 val loss 6541.507842616031\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 56 \n",
      "Epoch 201: train loss 891.5426584574991 val loss 5675.205546730443\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 57 \n",
      "Epoch 202: train loss 852.6323363709593 val loss 6347.836551063939\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 58 \n",
      "Epoch 203: train loss 910.5570044146326 val loss 5489.988767121968\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 59 \n",
      "Epoch 204: train loss 909.5122579243368 val loss 4726.468025609067\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 205: train loss 873.8319713569687 val loss 5603.536581340589\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 206: train loss 875.4518414800038 val loss 4664.991803620991\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 207: train loss 890.2760171490515 val loss 6095.448746129086\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 208: train loss 869.2040695830019 val loss 6661.642552425987\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 209: train loss 853.833837726159 val loss 6366.508862545616\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 210: train loss 871.9762436187195 val loss 4734.231121264006\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 211: train loss 863.8648132506959 val loss 4535.991801211709\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 212: train loss 829.0577905620644 val loss 6733.1830086959035\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 213: train loss 810.7083852162618 val loss 5197.488575182463\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 214: train loss 829.7520355864199 val loss 6766.382559525339\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 215: train loss 1266.3961309558617 val loss 5722.345838044819\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 216: train loss 1055.0849818155436 val loss 4686.330726623535\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 217: train loss 892.5421505773853 val loss 5916.445559853001\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 218: train loss 828.4589591568815 val loss 6249.440926802786\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 219: train loss 796.5710413584452 val loss 6329.61688071803\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 220: train loss 806.7698312633765 val loss 6429.599986026162\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 221: train loss 784.0851587078529 val loss 6086.713282534951\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 222: train loss 784.9716246827634 val loss 6124.305467705977\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 223: train loss 797.895263854615 val loss 5733.119370711477\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 12 \n",
      "Epoch 224: train loss 786.1127537710224 val loss 4659.607854341206\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 13 \n",
      "Epoch 225: train loss 795.946207469095 val loss 4978.719041924727\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 14 \n",
      "Epoch 226: train loss 808.4379330480884 val loss 4696.195321735583\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 15 \n",
      "Epoch 227: train loss 799.999479373772 val loss 4685.12167960719\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 16 \n",
      "Epoch 228: train loss 789.7983219809161 val loss 4727.178072076094\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 17 \n",
      "Epoch 229: train loss 910.7498107270567 val loss 5443.6131013569075\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 18 \n",
      "Epoch 230: train loss 914.5686477387023 val loss 6567.27518704063\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 19 \n",
      "Epoch 231: train loss 929.5344215438752 val loss 4686.473901447497\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 20 \n",
      "Epoch 232: train loss 832.8316878815612 val loss 4862.698173924497\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 21 \n",
      "Epoch 233: train loss 776.3195104085042 val loss 4680.3958953054325\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 22 \n",
      "Epoch 234: train loss 749.5897469434909 val loss 4638.5206491570725\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 23 \n",
      "Epoch 235: train loss 729.2784792506052 val loss 4764.323024147435\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 24 \n",
      "Epoch 236: train loss 727.8761870789671 val loss 4694.99005287572\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 25 \n",
      "Epoch 237: train loss 728.0177645197886 val loss 4886.475671065481\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 26 \n",
      "Epoch 238: train loss 745.6588032431231 val loss 4869.258082741185\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 27 \n",
      "Epoch 239: train loss 953.2464082912056 val loss 6157.350036621094\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 28 \n",
      "Epoch 240: train loss 877.5926207125544 val loss 6309.861534921746\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 29 \n",
      "Epoch 241: train loss 845.183366055974 val loss 6312.421382301732\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 30 \n",
      "Epoch 242: train loss 779.1749292704874 val loss 6197.005239386308\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 31 \n",
      "Epoch 243: train loss 805.9745899131912 val loss 5037.175169693796\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 32 \n",
      "Epoch 244: train loss 832.468001953856 val loss 4536.152418839304\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 33 \n",
      "Epoch 245: train loss 841.6483106327628 val loss 4665.681365163703\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 34 \n",
      "Epoch 246: train loss 796.4040899676477 val loss 5927.711701644094\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 35 \n",
      "Epoch 247: train loss 787.842953253649 val loss 5224.40372185958\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 36 \n",
      "Epoch 248: train loss 736.6869679525227 val loss 4601.006099901701\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 37 \n",
      "Epoch 249: train loss 752.406444069868 val loss 4545.826436494526\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 38 \n",
      "Epoch 250: train loss 945.6731075812243 val loss 4535.9280540064765\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 251: train loss 853.2099567801653 val loss 4582.042782432155\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 252: train loss 714.1812065261566 val loss 4468.884518271999\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 253: train loss 943.2228104368655 val loss 4487.134794937937\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 254: train loss 782.2872661202254 val loss 5745.599036768863\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 255: train loss 796.7886870606931 val loss 5656.445609243293\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 256: train loss 714.6222544230387 val loss 4862.874774732088\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 257: train loss 686.8227535864551 val loss 5026.63110833419\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 258: train loss 682.0490724940499 val loss 5576.831964593184\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 259: train loss 674.5840567400355 val loss 5015.459636889006\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 260: train loss 675.0556874989036 val loss 5005.137444747122\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 261: train loss 724.2212457257117 val loss 5029.858425742702\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 262: train loss 734.6477931587996 val loss 4775.367911087839\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 263: train loss 708.3873995478282 val loss 4944.95130799946\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 264: train loss 786.2777620418343 val loss 5713.6607433118315\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 12 \n",
      "Epoch 265: train loss 749.6274701877983 val loss 5471.715235659951\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 13 \n",
      "Epoch 266: train loss 700.0851437688588 val loss 4912.104219537032\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 14 \n",
      "Epoch 267: train loss 682.0040417060167 val loss 5021.453507674368\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 15 \n",
      "Epoch 268: train loss 666.7896608364082 val loss 4776.270252428557\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 16 \n",
      "Epoch 269: train loss 664.0347356739159 val loss 5015.37799714741\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 17 \n",
      "Epoch 270: train loss 685.4442298112515 val loss 5133.723340887773\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 18 \n",
      "Epoch 271: train loss 792.5301537428073 val loss 4897.052537215383\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 19 \n",
      "Epoch 272: train loss 746.9185942689816 val loss 5406.677050138775\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 20 \n",
      "Epoch 273: train loss 712.5690791878158 val loss 5558.789620650442\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 21 \n",
      "Epoch 274: train loss 658.6608758800758 val loss 5624.920092934056\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 22 \n",
      "Epoch 275: train loss 641.4758241390754 val loss 5570.4072149176345\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 23 \n",
      "Epoch 276: train loss 657.909635532402 val loss 5240.442975897538\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 24 \n",
      "Epoch 277: train loss 735.1017040321213 val loss 4998.338782862613\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 25 \n",
      "Epoch 278: train loss 722.9515759131152 val loss 5419.807055423134\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 26 \n",
      "Epoch 279: train loss 732.4562868129707 val loss 4775.284905684622\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 27 \n",
      "Epoch 280: train loss 679.788118236793 val loss 5341.757138703999\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 28 \n",
      "Epoch 281: train loss 669.1382027814489 val loss 5139.162843403064\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 29 \n",
      "Epoch 282: train loss 647.0940476377567 val loss 5100.364730433414\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 30 \n",
      "Epoch 283: train loss 694.735694291349 val loss 5013.035060681795\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 31 \n",
      "Epoch 284: train loss 680.9019420418197 val loss 4790.110500938014\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 32 \n",
      "Epoch 285: train loss 660.2757058971656 val loss 4926.375863727771\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 33 \n",
      "Epoch 286: train loss 644.1131889206207 val loss 4919.356271442614\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 34 \n",
      "Epoch 287: train loss 770.2249411394496 val loss 5412.3934133429275\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 35 \n",
      "Epoch 288: train loss 732.2026439369796 val loss 5493.465968081826\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 36 \n",
      "Epoch 289: train loss 672.3971779189424 val loss 4606.693752088045\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 37 \n",
      "Epoch 290: train loss 635.607210536203 val loss 4625.172433953536\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 38 \n",
      "Epoch 291: train loss 617.6671264100218 val loss 4608.052385430587\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 39 \n",
      "Epoch 292: train loss 603.2019172714142 val loss 4631.4397687410055\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 40 \n",
      "Epoch 293: train loss 599.1591251852983 val loss 4667.775798596834\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 41 \n",
      "Epoch 294: train loss 626.9838249526338 val loss 4542.945450230649\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 42 \n",
      "Epoch 295: train loss 656.594303519426 val loss 4934.415865847939\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 43 \n",
      "Epoch 296: train loss 656.235807178977 val loss 4639.362206710012\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 44 \n",
      "Epoch 297: train loss 723.5300253679652 val loss 4620.707690188759\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 45 \n",
      "Epoch 298: train loss 783.025099154718 val loss 4668.0101105539425\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 46 \n",
      "Epoch 299: train loss 648.3960432863521 val loss 5038.460130189595\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 47 \n",
      "Epoch 300: train loss 623.2864793332037 val loss 5341.188405890214\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 48 \n",
      "Epoch 301: train loss 668.1226935929167 val loss 5978.893539428711\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 49 \n",
      "Epoch 302: train loss 597.3915457354334 val loss 5900.986149035002\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 50 \n",
      "Epoch 303: train loss 586.9067985397613 val loss 4594.5020113493265\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 51 \n",
      "Epoch 304: train loss 623.1522399536864 val loss 4589.063111154656\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 52 \n",
      "Epoch 305: train loss 607.4764591605364 val loss 4772.424272637618\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 53 \n",
      "Epoch 306: train loss 602.6550204796705 val loss 4788.903976841977\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 54 \n",
      "Epoch 307: train loss 588.760969927211 val loss 4546.9492962485865\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 55 \n",
      "Epoch 308: train loss 606.8029201758836 val loss 4841.113855060778\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 56 \n",
      "Epoch 309: train loss 588.8038864135742 val loss 4746.852894431667\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 57 \n",
      "Epoch 310: train loss 579.1532432875947 val loss 4833.870145295796\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 58 \n",
      "Epoch 311: train loss 623.2676325358317 val loss 5614.829554507607\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 59 \n",
      "Epoch 312: train loss 630.0674053352036 val loss 5530.992683009097\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 60 \n",
      "Epoch 313: train loss 638.6178032423921 val loss 4910.548350685521\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 61 \n",
      "Epoch 314: train loss 616.5912015538016 val loss 4573.773541099147\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 62 \n",
      "Epoch 315: train loss 583.2071189651946 val loss 4487.041064613743\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 63 \n",
      "Epoch 316: train loss 599.9215545654297 val loss 5241.599329095137\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 64 \n",
      "Epoch 317: train loss 657.2685810934283 val loss 4778.140367608321\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 65 \n",
      "Epoch 318: train loss 607.0021071748106 val loss 4557.84253130461\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 66 \n",
      "Epoch 319: train loss 576.5091163955049 val loss 5174.267574511076\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 67 \n",
      "Epoch 320: train loss 613.9116250226598 val loss 4811.830911736739\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 68 \n",
      "Epoch 321: train loss 586.247867561386 val loss 4714.49113665129\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 69 \n",
      "Epoch 322: train loss 588.6831968387444 val loss 4596.362271359092\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 70 \n",
      "Epoch 323: train loss 584.5171933202687 val loss 5160.951897470574\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 71 \n",
      "Epoch 324: train loss 563.6711685272033 val loss 4688.111424094753\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 72 \n",
      "Epoch 325: train loss 576.862158586879 val loss 4612.423859445672\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 73 \n",
      "Epoch 326: train loss 574.7068015458341 val loss 4481.913323653372\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 74 \n",
      "Epoch 327: train loss 595.7891887276472 val loss 5894.95802708676\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 75 \n",
      "Epoch 328: train loss 561.6778781000012 val loss 4667.7662739000825\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 76 \n",
      "Epoch 329: train loss 546.7149345032469 val loss 4739.9138239810345\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 77 \n",
      "Epoch 330: train loss 538.3771430832184 val loss 4595.325993588096\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 78 \n",
      "Epoch 331: train loss 562.8332112477925 val loss 4669.091042769583\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 79 \n",
      "Epoch 332: train loss 567.3095704723975 val loss 4727.678359985352\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 80 \n",
      "Epoch 333: train loss 558.355346291365 val loss 4659.510714882298\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 81 \n",
      "Epoch 334: train loss 557.1836919270589 val loss 4563.36682169061\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 82 \n",
      "Epoch 335: train loss 582.2530301944938 val loss 4796.164580495734\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 83 \n",
      "Epoch 336: train loss 1261.5208054502566 val loss 4743.9117837203175\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 84 \n",
      "Epoch 337: train loss 791.7176046314354 val loss 5430.199687355443\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 85 \n",
      "Epoch 338: train loss 666.1896287381292 val loss 4868.134657207288\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 86 \n",
      "Epoch 339: train loss 590.9053271630567 val loss 6048.456517671284\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 87 \n",
      "Epoch 340: train loss 556.1078722833873 val loss 5239.878836380808\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 88 \n",
      "Epoch 341: train loss 541.78849257966 val loss 4640.8833650287825\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 89 \n",
      "Epoch 342: train loss 529.8810569626129 val loss 4602.7333655106395\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 90 \n",
      "Epoch 343: train loss 538.7309031686383 val loss 4751.202483729312\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 91 \n",
      "Epoch 344: train loss 572.4726457881357 val loss 5277.54077349211\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 92 \n",
      "Epoch 345: train loss 538.6258950376225 val loss 5765.591208608527\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 93 \n",
      "Epoch 346: train loss 533.5980129356155 val loss 5555.981485065661\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 94 \n",
      "Epoch 347: train loss 517.069172613635 val loss 5718.902384707802\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 95 \n",
      "Epoch 348: train loss 501.4957572114682 val loss 5080.94758605957\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 96 \n",
      "Epoch 349: train loss 534.0669357391174 val loss 5004.292754725406\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 97 \n",
      "Epoch 350: train loss 663.2504348754883 val loss 5709.507308156867\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 98 \n",
      "Epoch 351: train loss 758.2199669112703 val loss 4559.3140636243315\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 99 \n",
      "Epoch 352: train loss 649.2248166021473 val loss 4631.461701644094\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 100 \n",
      "Epoch 353: train loss 609.0305295019093 val loss 5252.999302914268\n",
      "Epoch 00353: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 101 \n",
      "Epoch 354: train loss 693.9497471426776 val loss 5068.054540533769\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 102 \n",
      "Epoch 355: train loss 606.449722335724 val loss 4960.910854540373\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 103 \n",
      "Epoch 356: train loss 568.1402703473668 val loss 4918.148016277112\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 104 \n",
      "Epoch 357: train loss 541.2855530013582 val loss 4788.304089997944\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 105 \n",
      "Epoch 358: train loss 521.2068751443646 val loss 4807.994261089124\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 106 \n",
      "Epoch 359: train loss 512.4691806267836 val loss 4833.884452819824\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 107 \n",
      "Epoch 360: train loss 504.69529513970105 val loss 4853.586557488692\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 108 \n",
      "Epoch 361: train loss 495.3359605252386 val loss 4843.368452774851\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 109 \n",
      "Epoch 362: train loss 489.23393964481926 val loss 4830.462194342363\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 110 \n",
      "Epoch 363: train loss 479.2223091353913 val loss 4827.528611434133\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 111 \n",
      "Epoch 364: train loss 473.5174414811734 val loss 4823.004271657844\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 112 \n",
      "Epoch 365: train loss 468.365487355672 val loss 4850.756104318719\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 113 \n",
      "Epoch 366: train loss 463.1277591454055 val loss 4875.786544799805\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 114 \n",
      "Epoch 367: train loss 457.9172800943523 val loss 4819.048074822676\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 115 \n",
      "Epoch 368: train loss 453.6440007786551 val loss 4803.971309862639\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 116 \n",
      "Epoch 369: train loss 450.368172902547 val loss 4767.636936388518\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 117 \n",
      "Epoch 370: train loss 448.53806697822614 val loss 4797.574806213379\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 118 \n",
      "Epoch 371: train loss 444.0734594653467 val loss 4706.223607916581\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 119 \n",
      "Epoch 372: train loss 441.1363010520707 val loss 4736.041615536338\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 120 \n",
      "Epoch 373: train loss 437.9852505758137 val loss 4856.292689273232\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 121 \n",
      "Epoch 374: train loss 435.64898695346125 val loss 4902.182887027138\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 122 \n",
      "Epoch 375: train loss 433.2982527453029 val loss 5082.27486379523\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 123 \n",
      "Epoch 376: train loss 436.6082394308673 val loss 4924.567837765342\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 124 \n",
      "Epoch 377: train loss 436.01917376489695 val loss 4876.298622131348\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 125 \n",
      "Epoch 378: train loss 437.3652165350086 val loss 4882.155333669562\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 126 \n",
      "Epoch 379: train loss 432.54019964526515 val loss 4762.226244474712\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 127 \n",
      "Epoch 380: train loss 433.25044796424 val loss 4739.124561109041\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 128 \n",
      "Epoch 381: train loss 429.35638519104367 val loss 4756.566690545333\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 129 \n",
      "Epoch 382: train loss 424.29077717215716 val loss 4781.809326573422\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 130 \n",
      "Epoch 383: train loss 422.02569913578606 val loss 4758.037457516319\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 131 \n",
      "Epoch 384: train loss 420.7242724253032 val loss 4785.152131331594\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 132 \n",
      "Epoch 385: train loss 419.8855535313041 val loss 4820.226652848093\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 133 \n",
      "Epoch 386: train loss 416.8825785699719 val loss 4856.3263638145045\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 134 \n",
      "Epoch 387: train loss 415.7670388592931 val loss 4841.486712004009\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 135 \n",
      "Epoch 388: train loss 413.7008399734954 val loss 4854.469925328305\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 136 \n",
      "Epoch 389: train loss 411.55026923539396 val loss 4865.787953828511\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 137 \n",
      "Epoch 390: train loss 410.78830830922385 val loss 4871.180942334627\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 138 \n",
      "Epoch 391: train loss 408.0461308599232 val loss 4716.070563868472\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 139 \n",
      "Epoch 392: train loss 408.22661154284447 val loss 4958.438933523078\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 140 \n",
      "Epoch 393: train loss 406.32461138970837 val loss 4952.198419671309\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 141 \n",
      "Epoch 394: train loss 409.2675998710587 val loss 4855.085824263723\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 142 \n",
      "Epoch 395: train loss 405.5402079599346 val loss 4880.332030848453\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 143 \n",
      "Epoch 396: train loss 402.4414536253421 val loss 4869.726821497867\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 144 \n",
      "Epoch 397: train loss 401.3065797497412 val loss 4905.421992653294\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 145 \n",
      "Epoch 398: train loss 404.5714924064225 val loss 4885.249439841823\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 146 \n",
      "Epoch 399: train loss 402.6945948115366 val loss 4873.79012298584\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 147 \n",
      "Epoch 400: train loss 401.8154476417039 val loss 4794.551514876516\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 148 \n",
      "Epoch 401: train loss 398.7839864399619 val loss 4897.510417335911\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 149 \n",
      "Epoch 402: train loss 396.5958187080429 val loss 4921.764613904451\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 150 \n",
      "Epoch 403: train loss 396.61012284056153 val loss 4955.866541410747\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 151 \n",
      "Epoch 404: train loss 395.5572040123854 val loss 4780.447497719212\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 152 \n",
      "Epoch 405: train loss 393.67586001093514 val loss 4820.606568587454\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 153 \n",
      "Epoch 406: train loss 392.7314839391651 val loss 4754.918958162007\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 154 \n",
      "Epoch 407: train loss 391.9412397281852 val loss 4914.932285911159\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 155 \n",
      "Epoch 408: train loss 390.32998115859345 val loss 4808.861036200273\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 156 \n",
      "Epoch 409: train loss 389.42024315474276 val loss 4886.067852221037\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 157 \n",
      "Epoch 410: train loss 389.6165575952587 val loss 4801.622144197163\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 158 \n",
      "Epoch 411: train loss 389.54137000352324 val loss 4819.348256964433\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 159 \n",
      "Epoch 412: train loss 387.1937595984179 val loss 4771.980672735917\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 160 \n",
      "Epoch 413: train loss 385.29006105982614 val loss 4769.7938485396535\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 161 \n",
      "Epoch 414: train loss 384.873201769983 val loss 4781.508545724969\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 162 \n",
      "Epoch 415: train loss 384.00618803286983 val loss 4780.741618507786\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 163 \n",
      "Epoch 416: train loss 383.1149640797141 val loss 4775.322620994167\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 164 \n",
      "Epoch 417: train loss 382.19864554034024 val loss 4777.355377598813\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 165 \n",
      "Epoch 418: train loss 381.44707977843143 val loss 4783.9643052753645\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 166 \n",
      "Epoch 419: train loss 383.6486256993459 val loss 4777.763498808208\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 167 \n",
      "Epoch 420: train loss 380.9555242846826 val loss 4789.297961184853\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 168 \n",
      "Epoch 421: train loss 378.947379472013 val loss 4775.848366185239\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 169 \n",
      "Epoch 422: train loss 377.9356964248383 val loss 4777.888559843364\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 170 \n",
      "Epoch 423: train loss 376.88635329286495 val loss 4784.255889892578\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 171 \n",
      "Epoch 424: train loss 378.3989309664972 val loss 4778.019752100894\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 172 \n",
      "Epoch 425: train loss 376.88958575768385 val loss 4793.13876945094\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 173 \n",
      "Epoch 426: train loss 377.36121060034475 val loss 4762.673352291709\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 174 \n",
      "Epoch 427: train loss 375.83741463301425 val loss 4767.158711082057\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 175 \n",
      "Epoch 428: train loss 374.7003582023575 val loss 4766.32916340075\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 176 \n",
      "Epoch 429: train loss 372.51821412868844 val loss 4775.800639102334\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 177 \n",
      "Epoch 430: train loss 371.3191601644733 val loss 4775.996981972142\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 178 \n",
      "Epoch 431: train loss 371.4530666374161 val loss 4776.8690904316145\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 179 \n",
      "Epoch 432: train loss 370.0995588473931 val loss 4772.891628867702\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 180 \n",
      "Epoch 433: train loss 370.1722362998003 val loss 4781.121976350483\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 181 \n",
      "Epoch 434: train loss 368.6552220875632 val loss 4781.670741031045\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 182 \n",
      "Epoch 435: train loss 370.0936396478893 val loss 4773.453006945158\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 183 \n",
      "Epoch 436: train loss 370.354500616382 val loss 4777.096364473042\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 184 \n",
      "Epoch 437: train loss 367.8748700947105 val loss 4776.41604172556\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 185 \n",
      "Epoch 438: train loss 366.4915431131146 val loss 4771.782175164473\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 186 \n",
      "Epoch 439: train loss 366.67863329584725 val loss 4787.5722732543945\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 187 \n",
      "Epoch 440: train loss 366.4599962291603 val loss 4775.705078526547\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 188 \n",
      "Epoch 441: train loss 365.05206938418087 val loss 4775.201732836272\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 189 \n",
      "Epoch 442: train loss 363.81883851925056 val loss 4783.185531214664\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 190 \n",
      "Epoch 443: train loss 363.22259653970866 val loss 4785.587569788882\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 191 \n",
      "Epoch 444: train loss 361.5862048942886 val loss 4763.443076685855\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 192 \n",
      "Epoch 445: train loss 361.4967057005374 val loss 4779.679041410747\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 193 \n",
      "Epoch 446: train loss 360.7368836545659 val loss 4772.012856332879\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 194 \n",
      "Epoch 447: train loss 360.7526682094186 val loss 4772.095651325427\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 195 \n",
      "Epoch 448: train loss 359.86204652158085 val loss 4773.963335539165\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 196 \n",
      "Epoch 449: train loss 358.3969690471352 val loss 4784.832565709165\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 197 \n",
      "Epoch 450: train loss 358.5812805998111 val loss 4757.506213539525\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 198 \n",
      "Epoch 451: train loss 358.7481585519756 val loss 4781.457939147949\n",
      "Current learning rate: 0.0001\n",
      "early_stopping_counter: 199 \n",
      "Epoch 452: train loss 363.6913424509014 val loss 4773.750000803094\n",
      "Current learning rate: 0.0001\n",
      "Early stopping at epoch 452 due to no improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model = model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    for seq_true, _ in data_loaders['train_x']:\n",
    "        seq_true = seq_true.to(device)\n",
    "        mask = ~torch.all(seq_true==0, axis=2)\n",
    "        seq_pred = model(seq_true)\n",
    "\n",
    "        if loss_reduction == 'stay_wise_mean':\n",
    "            l = criterion(seq_pred[mask], seq_true[mask]).sum(axis=1)\n",
    "            lens = mask.sum(axis=1).detach().cpu()\n",
    "            c_lens = lens.cumsum(dim=0)\n",
    "            loss = 0\n",
    "            for idx, i in enumerate(c_lens):\n",
    "                s = 0 if idx == 0 else c_lens[idx-1]\n",
    "                loss += l[s:i].sum()/lens[idx]\n",
    "        elif loss_reduction == 'global_mean':\n",
    "            l = criterion(seq_pred[mask], seq_true[mask]).sum()\n",
    "            loss = l/mask.sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        #print(len(train_losses))\n",
    "\n",
    "    val_losses = []\n",
    "    model = model.eval()\n",
    "    with torch.no_grad():\n",
    "        for seq_true, _ in data_loaders['val_x']:\n",
    "            seq_true = seq_true.to(device)\n",
    "            mask = ~torch.all(seq_true==0, axis=2)\n",
    "            seq_pred = model(seq_true)\n",
    "\n",
    "            if loss_reduction == 'stay_wise_mean':\n",
    "                l = criterion(seq_pred[mask], seq_true[mask]).sum(axis=1)\n",
    "                lens = mask.sum(axis=1).detach().cpu()\n",
    "                c_lens = lens.cumsum(dim=0)\n",
    "                loss = 0\n",
    "                for idx, i in enumerate(c_lens):\n",
    "                    s = 0 if idx == 0 else c_lens[idx-1]\n",
    "                    loss += l[s:i].sum()/lens[idx]\n",
    "            elif loss_reduction == 'global_mean':\n",
    "                l = criterion(seq_pred[mask], seq_true[mask]).sum()\n",
    "                loss = l/mask.sum()\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    #print(len(train_losses))\n",
    "    val_loss = np.mean(val_losses)\n",
    "\n",
    "    history['train'].append(train_loss)\n",
    "    history['val'].append(val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(\"Current learning rate:\", optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= early_patience:\n",
    "            print(f'Early stopping at epoch {epoch} due to no improvement in validation loss.')\n",
    "            break\n",
    "    \n",
    "    print(f'early_stopping_counter: {early_stopping_counter} ')\n",
    "    \n",
    "\n",
    "model.load_state_dict(best_model_wts)\n",
    "torch.save(model.state_dict(), f'best_moel-{data}-hos-{loss_reduction}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecurrentAutoencoder(\n",
       "  (encoder): Encoder(\n",
       "    (rnn1): LSTM(383, 1024, batch_first=True)\n",
       "    (rnn2): LSTM(1024, 512, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (rnn1): LSTM(512, 1024, batch_first=True)\n",
       "    (output_layer): Linear(in_features=1024, out_features=383, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13585/13585 [01:51<00:00, 121.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# evaluation data\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "seed_everything(SEED)\n",
    "hidden_unit = 256\n",
    "\n",
    "model = RecurrentAutoencoder(83, 383, 512)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(f'best_moel-std-hos-global_mean.pth'))\n",
    "model = model.eval()\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean').to(device)\n",
    "\n",
    "# loss calculation\n",
    "eval_split = 'val_test_x' #tst val_th\n",
    "\n",
    "eval_data = []\n",
    "with torch.no_grad():\n",
    "    for seq_true, stay_id in tqdm(data_loaders[eval_split]):\n",
    "        s_id = stay_id.cpu().numpy().ravel()[0]\n",
    "        seq_true = seq_true.to(device)\n",
    "        seq_pred = model(seq_true)\n",
    "        loss=criterion(seq_pred, seq_true)\n",
    "        \n",
    "        eval_data.append([s_id,loss.item()])\n",
    "\n",
    "eval_data = pd.DataFrame(eval_data, columns=['hadm_id', 'score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hos_data=pd.read_csv('hos_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20004038</td>\n",
       "      <td>0.533074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20004038</td>\n",
       "      <td>0.515734</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20004038</td>\n",
       "      <td>0.596503</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20004038</td>\n",
       "      <td>0.595708</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20004038</td>\n",
       "      <td>0.487797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13580</th>\n",
       "      <td>29996041</td>\n",
       "      <td>0.550587</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13581</th>\n",
       "      <td>29996041</td>\n",
       "      <td>0.537918</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13582</th>\n",
       "      <td>29996041</td>\n",
       "      <td>0.533166</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13583</th>\n",
       "      <td>29996041</td>\n",
       "      <td>0.539521</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13584</th>\n",
       "      <td>29996041</td>\n",
       "      <td>0.527081</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13585 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        hadm_id     score  label\n",
       "0      20004038  0.533074      0\n",
       "1      20004038  0.515734      0\n",
       "2      20004038  0.596503      0\n",
       "3      20004038  0.595708      0\n",
       "4      20004038  0.487797      0\n",
       "...         ...       ...    ...\n",
       "13580  29996041  0.550587      0\n",
       "13581  29996041  0.537918      0\n",
       "13582  29996041  0.533166      0\n",
       "13583  29996041  0.539521      0\n",
       "13584  29996041  0.527081      0\n",
       "\n",
       "[13585 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_d=hos_data[hos_data['hadm_id'].isin(eval_data['hadm_id'])]\n",
    "eval_data_1=pd.merge(eval_data,i_d,on='hadm_id',how='left')\n",
    "eval_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_split='val_test_x'\n",
    "eval_data_1.to_csv(f\"eval_{data}-data-hos-{eval_split}-{loss_reduction}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14340/14340 [02:00<00:00, 118.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# evaluation data\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "seed_everything(SEED)\n",
    "hidden_unit = 512\n",
    "\n",
    "model = RecurrentAutoencoder(83, 383, hidden_unit)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(f'best_moel-{data}-hos-{loss_reduction}.pth'))\n",
    "model = model.eval()\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean').to(device)\n",
    "\n",
    "# loss calculation\n",
    "eval_split = 'test_x' #tst val_th\n",
    "\n",
    "eval_data_2 = []\n",
    "with torch.no_grad():\n",
    "    for seq_true, hadm_id in tqdm(data_loaders[eval_split]):\n",
    "        s_id = hadm_id.cpu().numpy().ravel()[0]\n",
    "        seq_true = seq_true.to(device)\n",
    "        seq_pred = model(seq_true)\n",
    "        loss=criterion(seq_pred, seq_true)\n",
    "        \n",
    "        eval_data_2.append([s_id,loss.item()])\n",
    "\n",
    "eval_data_2 = pd.DataFrame(eval_data_2, columns=['hadm_id', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20022720</td>\n",
       "      <td>0.463378</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20022720</td>\n",
       "      <td>0.492806</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20022720</td>\n",
       "      <td>0.481417</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20022720</td>\n",
       "      <td>0.466180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20022720</td>\n",
       "      <td>0.461356</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14335</th>\n",
       "      <td>29997500</td>\n",
       "      <td>0.394761</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14336</th>\n",
       "      <td>29997500</td>\n",
       "      <td>0.390293</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14337</th>\n",
       "      <td>29997500</td>\n",
       "      <td>0.384276</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14338</th>\n",
       "      <td>29997500</td>\n",
       "      <td>0.376598</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14339</th>\n",
       "      <td>29997500</td>\n",
       "      <td>0.373914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14340 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        hadm_id     score  label\n",
       "0      20022720  0.463378      0\n",
       "1      20022720  0.492806      0\n",
       "2      20022720  0.481417      0\n",
       "3      20022720  0.466180      0\n",
       "4      20022720  0.461356      0\n",
       "...         ...       ...    ...\n",
       "14335  29997500  0.394761      1\n",
       "14336  29997500  0.390293      1\n",
       "14337  29997500  0.384276      1\n",
       "14338  29997500  0.376598      1\n",
       "14339  29997500  0.373914      1\n",
       "\n",
       "[14340 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_d2=hos_data[hos_data['hadm_id'].isin(eval_data_2['hadm_id'])]\n",
    "eval_data_2=pd.merge(eval_data_2,i_d2,on='hadm_id',how='left')\n",
    "eval_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_split='test_x_6_hour'\n",
    "eval_data_2.to_csv(f\"eval_{data}-data-hos-{eval_split}-{loss_reduction}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13567/13567 [03:06<00:00, 72.83it/s]\n"
     ]
    }
   ],
   "source": [
    "def conf_mat(true, pred):\n",
    "    tp = ((pred == 1) & (true == 1)).sum()\n",
    "    fp = ((pred == 1) & (true == 0)).sum()\n",
    "    fn = ((pred == 0) & (true == 1)).sum()\n",
    "    tn = ((pred == 0) & (true == 0)).sum()\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "eval_result = []\n",
    "\n",
    "scores = eval_data_1['score'].unique()\n",
    "\n",
    "for s in tqdm(scores):\n",
    "    eval_data_1['pred'] = np.where(eval_data_1['score']>=s, 1, 0)\n",
    "    tmp = eval_data_1.groupby('hadm_id').agg({'label': lambda x: x.values[0], 'pred': 'max'}).reset_index()\n",
    "    tp, fp, fn, tn = conf_mat(tmp['label'], tmp['pred'])\n",
    "\n",
    "    eval_result.append([s, tp/(tp+fn), tp/(tp+fp), 2*tp/(fp+2*tp+fn)])\n",
    "\n",
    "eval_result = pd.DataFrame(eval_result, columns=['score', 'rec', 'prec', 'f1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_split='val_test_x_6_hour'\n",
    "eval_result.to_csv(f\"eval_reult_{data}-data-hos-{eval_split}-{loss_reduction}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>rec</th>\n",
       "      <th>prec</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.533074</td>\n",
       "      <td>0.950431</td>\n",
       "      <td>0.507480</td>\n",
       "      <td>0.661665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.515734</td>\n",
       "      <td>0.987069</td>\n",
       "      <td>0.498368</td>\n",
       "      <td>0.662328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.596503</td>\n",
       "      <td>0.607759</td>\n",
       "      <td>0.692875</td>\n",
       "      <td>0.647532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.595708</td>\n",
       "      <td>0.607759</td>\n",
       "      <td>0.692875</td>\n",
       "      <td>0.647532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.487797</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13562</th>\n",
       "      <td>0.550587</td>\n",
       "      <td>0.797414</td>\n",
       "      <td>0.558069</td>\n",
       "      <td>0.656610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13563</th>\n",
       "      <td>0.537918</td>\n",
       "      <td>0.905172</td>\n",
       "      <td>0.520446</td>\n",
       "      <td>0.660897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13564</th>\n",
       "      <td>0.533166</td>\n",
       "      <td>0.950431</td>\n",
       "      <td>0.508065</td>\n",
       "      <td>0.662162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13565</th>\n",
       "      <td>0.539521</td>\n",
       "      <td>0.887931</td>\n",
       "      <td>0.526854</td>\n",
       "      <td>0.661316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13566</th>\n",
       "      <td>0.527081</td>\n",
       "      <td>0.978448</td>\n",
       "      <td>0.500551</td>\n",
       "      <td>0.662290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13567 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          score       rec      prec        f1\n",
       "0      0.533074  0.950431  0.507480  0.661665\n",
       "1      0.515734  0.987069  0.498368  0.662328\n",
       "2      0.596503  0.607759  0.692875  0.647532\n",
       "3      0.595708  0.607759  0.692875  0.647532\n",
       "4      0.487797  1.000000  0.500000  0.666667\n",
       "...         ...       ...       ...       ...\n",
       "13562  0.550587  0.797414  0.558069  0.656610\n",
       "13563  0.537918  0.905172  0.520446  0.660897\n",
       "13564  0.533166  0.950431  0.508065  0.662162\n",
       "13565  0.539521  0.887931  0.526854  0.661316\n",
       "13566  0.527081  0.978448  0.500551  0.662290\n",
       "\n",
       "[13567 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>rec</th>\n",
       "      <th>prec</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.487797</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.486028</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.442199</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.455245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.427778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13525</th>\n",
       "      <td>0.468516</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13526</th>\n",
       "      <td>0.467816</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13527</th>\n",
       "      <td>0.463314</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13528</th>\n",
       "      <td>0.452889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13559</th>\n",
       "      <td>0.499482</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6128 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          score  rec  prec        f1\n",
       "4      0.487797  1.0   0.5  0.666667\n",
       "5      0.486028  1.0   0.5  0.666667\n",
       "6      0.442199  1.0   0.5  0.666667\n",
       "7      0.455245  1.0   0.5  0.666667\n",
       "8      0.427778  1.0   0.5  0.666667\n",
       "...         ...  ...   ...       ...\n",
       "13525  0.468516  1.0   0.5  0.666667\n",
       "13526  0.467816  1.0   0.5  0.666667\n",
       "13527  0.463314  1.0   0.5  0.666667\n",
       "13528  0.452889  1.0   0.5  0.666667\n",
       "13559  0.499482  1.0   0.5  0.666667\n",
       "\n",
       "[6128 rows x 4 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result[eval_result['f1']>=eval_result['f1'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 7324/13567 [02:43<02:22, 43.67it/s]C:\\Users\\DAHS\\AppData\\Local\\Temp\\ipykernel_21520\\2314707552.py:18: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  eval_result.append([s, tp/(tp+fn), tp/(tp+fp), 2*tp/(fp+2*tp+fn)])\n",
      " 54%|█████▍    | 7329/13567 [02:43<02:24, 43.04it/s]C:\\Users\\DAHS\\AppData\\Local\\Temp\\ipykernel_21520\\2314707552.py:18: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  eval_result.append([s, tp/(tp+fn), tp/(tp+fp), 2*tp/(fp+2*tp+fn)])\n",
      "C:\\Users\\DAHS\\AppData\\Local\\Temp\\ipykernel_21520\\2314707552.py:18: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  eval_result.append([s, tp/(tp+fn), tp/(tp+fp), 2*tp/(fp+2*tp+fn)])\n",
      " 54%|█████▍    | 7335/13567 [02:43<02:14, 46.33it/s]C:\\Users\\DAHS\\AppData\\Local\\Temp\\ipykernel_21520\\2314707552.py:18: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  eval_result.append([s, tp/(tp+fn), tp/(tp+fp), 2*tp/(fp+2*tp+fn)])\n",
      "100%|██████████| 13567/13567 [05:08<00:00, 43.91it/s]\n"
     ]
    }
   ],
   "source": [
    "def conf_mat(true, pred):\n",
    "    tp = ((pred == 1) & (true == 1)).sum()\n",
    "    fp = ((pred == 1) & (true == 0)).sum()\n",
    "    fn = ((pred == 0) & (true == 1)).sum()\n",
    "    tn = ((pred == 0) & (true == 0)).sum()\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "eval_result = []\n",
    "\n",
    "scores = eval_data_1['score'].unique()\n",
    "\n",
    "for s in tqdm(scores):\n",
    "    eval_data_1['pred'] = np.where(eval_data_1['score']>=s, 1, 0)\n",
    "    tmp = eval_data_1.groupby('hadm_id').agg(label=('label', lambda x: x.values[0]), alarm_num=('pred','sum')).reset_index()\n",
    "    tmp['pred'] = np.where(tmp['alarm_num'] >= 5, 1, 0)\n",
    "    tp, fp, fn, tn = conf_mat(tmp['label'], tmp['pred'])\n",
    "\n",
    "    eval_result.append([s, tp/(tp+fn), tp/(tp+fp), 2*tp/(fp+2*tp+fn)])\n",
    "\n",
    "eval_result = pd.DataFrame(eval_result, columns=['score', 'rec', 'prec', 'f1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>rec</th>\n",
       "      <th>prec</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>0.443247</td>\n",
       "      <td>0.469828</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.452752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>0.443305</td>\n",
       "      <td>0.469828</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.452752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>0.443317</td>\n",
       "      <td>0.469828</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.452752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>0.443278</td>\n",
       "      <td>0.469828</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.452752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8375</th>\n",
       "      <td>0.443301</td>\n",
       "      <td>0.469828</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.452752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9186</th>\n",
       "      <td>0.443391</td>\n",
       "      <td>0.469828</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.452752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9759</th>\n",
       "      <td>0.443427</td>\n",
       "      <td>0.469828</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.452752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9927</th>\n",
       "      <td>0.443427</td>\n",
       "      <td>0.469828</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.452752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10103</th>\n",
       "      <td>0.443249</td>\n",
       "      <td>0.469828</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.452752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10945</th>\n",
       "      <td>0.443329</td>\n",
       "      <td>0.469828</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.452752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11356</th>\n",
       "      <td>0.443242</td>\n",
       "      <td>0.469828</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.452752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12043</th>\n",
       "      <td>0.443253</td>\n",
       "      <td>0.469828</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.452752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12050</th>\n",
       "      <td>0.443404</td>\n",
       "      <td>0.469828</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.452752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          score       rec      prec        f1\n",
       "1580   0.443247  0.469828  0.436874  0.452752\n",
       "1625   0.443305  0.469828  0.436874  0.452752\n",
       "2859   0.443317  0.469828  0.436874  0.452752\n",
       "5620   0.443278  0.469828  0.436874  0.452752\n",
       "8375   0.443301  0.469828  0.436874  0.452752\n",
       "9186   0.443391  0.469828  0.436874  0.452752\n",
       "9759   0.443427  0.469828  0.436874  0.452752\n",
       "9927   0.443427  0.469828  0.436874  0.452752\n",
       "10103  0.443249  0.469828  0.436874  0.452752\n",
       "10945  0.443329  0.469828  0.436874  0.452752\n",
       "11356  0.443242  0.469828  0.436874  0.452752\n",
       "12043  0.443253  0.469828  0.436874  0.452752\n",
       "12050  0.443404  0.469828  0.436874  0.452752"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result[eval_result['f1']>=eval_result['f1'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 7327/13567 [01:47<01:29, 69.43it/s]C:\\Users\\DAHS\\AppData\\Local\\Temp\\ipykernel_21520\\809627442.py:19: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  eval_result.append([s, tp/(tp+fn), tp/(tp+fp), 2*tp/(fp+2*tp+fn)])\n",
      "100%|██████████| 13567/13567 [03:18<00:00, 68.29it/s]\n"
     ]
    }
   ],
   "source": [
    "def conf_mat(true, pred):\n",
    "    tp = ((pred == 1) & (true == 1)).sum()\n",
    "    fp = ((pred == 1) & (true == 0)).sum()\n",
    "    fn = ((pred == 0) & (true == 1)).sum()\n",
    "    tn = ((pred == 0) & (true == 0)).sum()\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "eval_result = []\n",
    "\n",
    "#scores = eval_data_1['score'].unique()\n",
    "\n",
    "for s in tqdm(scores):\n",
    "    eval_data_1['pred'] = np.where(eval_data_1['score']>=s, 1, 0)\n",
    "    eval_data_1['pred_shifted'] = eval_data_1.groupby('hadm_id')['pred'].shift(1).fillna(0)    \n",
    "    eval_data_1['continuous_ones'] = ((eval_data_1['pred'] + eval_data_1['pred_shifted']) == 2).astype(int)\n",
    "    tmp = eval_data_1.groupby('hadm_id').agg({'label': lambda x: x.values[0], 'continuous_ones': 'max'}).reset_index()\n",
    "    tp, fp, fn, tn = conf_mat(tmp['label'], tmp['continuous_ones'])\n",
    "\n",
    "    eval_result.append([s, tp/(tp+fn), tp/(tp+fp), 2*tp/(fp+2*tp+fn)])\n",
    "\n",
    "eval_result = pd.DataFrame(eval_result, columns=['score', 'rec', 'prec', 'f1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>rec</th>\n",
       "      <th>prec</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>0.502423</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>0.502178</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569</th>\n",
       "      <td>0.502147</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>0.502420</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>0.502433</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>0.502097</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4080</th>\n",
       "      <td>0.502475</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>0.502347</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4740</th>\n",
       "      <td>0.502068</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6634</th>\n",
       "      <td>0.502095</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6685</th>\n",
       "      <td>0.502362</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7135</th>\n",
       "      <td>0.502322</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7156</th>\n",
       "      <td>0.502241</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7647</th>\n",
       "      <td>0.502397</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7745</th>\n",
       "      <td>0.502087</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8655</th>\n",
       "      <td>0.502361</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9505</th>\n",
       "      <td>0.502441</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9782</th>\n",
       "      <td>0.502189</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10118</th>\n",
       "      <td>0.502451</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10581</th>\n",
       "      <td>0.502387</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10670</th>\n",
       "      <td>0.502325</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10766</th>\n",
       "      <td>0.502318</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10980</th>\n",
       "      <td>0.502304</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11031</th>\n",
       "      <td>0.502461</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11262</th>\n",
       "      <td>0.502364</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11570</th>\n",
       "      <td>0.502256</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11790</th>\n",
       "      <td>0.502207</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12054</th>\n",
       "      <td>0.502046</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12466</th>\n",
       "      <td>0.502374</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12517</th>\n",
       "      <td>0.502362</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12659</th>\n",
       "      <td>0.502256</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12731</th>\n",
       "      <td>0.502254</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12891</th>\n",
       "      <td>0.502367</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13106</th>\n",
       "      <td>0.502028</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.649446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          score       rec      prec        f1\n",
       "350    0.502423  0.948276  0.493827  0.649446\n",
       "952    0.502178  0.948276  0.493827  0.649446\n",
       "1569   0.502147  0.948276  0.493827  0.649446\n",
       "2095   0.502420  0.948276  0.493827  0.649446\n",
       "2609   0.502433  0.948276  0.493827  0.649446\n",
       "3595   0.502097  0.948276  0.493827  0.649446\n",
       "4080   0.502475  0.948276  0.493827  0.649446\n",
       "4704   0.502347  0.948276  0.493827  0.649446\n",
       "4740   0.502068  0.948276  0.493827  0.649446\n",
       "6634   0.502095  0.948276  0.493827  0.649446\n",
       "6685   0.502362  0.948276  0.493827  0.649446\n",
       "7135   0.502322  0.948276  0.493827  0.649446\n",
       "7156   0.502241  0.948276  0.493827  0.649446\n",
       "7647   0.502397  0.948276  0.493827  0.649446\n",
       "7745   0.502087  0.948276  0.493827  0.649446\n",
       "8655   0.502361  0.948276  0.493827  0.649446\n",
       "9505   0.502441  0.948276  0.493827  0.649446\n",
       "9782   0.502189  0.948276  0.493827  0.649446\n",
       "10118  0.502451  0.948276  0.493827  0.649446\n",
       "10581  0.502387  0.948276  0.493827  0.649446\n",
       "10670  0.502325  0.948276  0.493827  0.649446\n",
       "10766  0.502318  0.948276  0.493827  0.649446\n",
       "10980  0.502304  0.948276  0.493827  0.649446\n",
       "11031  0.502461  0.948276  0.493827  0.649446\n",
       "11262  0.502364  0.948276  0.493827  0.649446\n",
       "11570  0.502256  0.948276  0.493827  0.649446\n",
       "11790  0.502207  0.948276  0.493827  0.649446\n",
       "12054  0.502046  0.948276  0.493827  0.649446\n",
       "12466  0.502374  0.948276  0.493827  0.649446\n",
       "12517  0.502362  0.948276  0.493827  0.649446\n",
       "12659  0.502256  0.948276  0.493827  0.649446\n",
       "12731  0.502254  0.948276  0.493827  0.649446\n",
       "12891  0.502367  0.948276  0.493827  0.649446\n",
       "13106  0.502028  0.948276  0.493827  0.649446"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result[eval_result['f1']>=eval_result['f1'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
